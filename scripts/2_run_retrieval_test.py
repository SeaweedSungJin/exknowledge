#2_run_retrieval_test.py
import os
import yaml
from PIL import Image

# saliency_vlm_core í´ë”ì˜ ëª¨ë“ˆë“¤ì„ ì„í¬íŠ¸í•˜ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from saliency_vlm_core.image_processing import pre_normalize_image_size
from saliency_vlm_core.saliency import SaliencyMapper
from saliency_vlm_core.retriever import WikiRetriever  
from saliency_vlm_core.llava_vqa import LLaVAVQA
from saliency_vlm_core.contriever_reranker import ContrieverReranker

# DEBUG í”Œë˜ê·¸: ì¶œë ¥ ë¬¸êµ¬ë¥¼ ì†ì‰½ê²Œ ë„ê¸° ìœ„í•œ ë³€ìˆ˜
VERBOSE = True

def main():
    # 1. ì„¤ì • íŒŒì¼ ë¡œë“œ
    print("--- ì„¤ì • íŒŒì¼ ë¡œë“œ ---")
    with open("configs/default_config.yaml", "r") as f:
        config = yaml.safe_load(f)
    print("ì„¤ì • ë¡œë“œ ì™„ë£Œ.\n")

    # WikiRetriever ê°ì²´ ìƒì„± (ëª¨ë¸, ë°ì´í„° ë¡œë”©) 
    # ì´ í•œ ì¤„ë¡œ ëª¨ë¸, í”„ë¡œì„¸ì„œ, FAISS ì¸ë±ìŠ¤, ìœ„í‚¤ ë°ì´í„°ê°€ ëª¨ë‘ ë¡œë“œë©ë‹ˆë‹¤.
    retriever = WikiRetriever(config)

    # ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ìµœì¢… ì¿¼ë¦¬ ë²¡í„° ìƒì„±
    print("--- ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ìµœì¢… ì¿¼ë¦¬ ë²¡í„° ìƒì„± ---")
    image_path = os.path.join(
        config["data_dir"], config["image_dir"], config["image_filename"]
    )
    original_image = Image.open(image_path).convert("RGB")
    
    # ì‚¬ì „ ì •ê·œí™”
    normalized_image = pre_normalize_image_size(
        original_image, target_size=config["pre_normalize_size"]
    )    
    # SaliencyMapper ê°ì²´ ìƒì„± ë° ì‹¤í–‰
    # ì´ì œ model, processor, deviceë¥¼ retriever ê°ì²´ë¡œë¶€í„° ê°€ì ¸ì˜µë‹ˆë‹¤.
    saliency_mapper = SaliencyMapper(
        retriever.model, retriever.processor, retriever.device
    )
    final_image_to_encode = saliency_mapper.get_saliency_cropped_image(
        image=normalized_image,
        query=config["saliency_query"],
        window_size=config["window_size"],
        stride=config["stride"],
        keep_top_percent=config["keep_top_percent"],
    )
    
    # ìµœì¢… ì´ë¯¸ì§€ë¥¼ retrieverë¥¼ í†µí•´ ì¸ì½”ë”©
    query_vector = retriever.encode_image(final_image_to_encode)
    print("ìµœì¢… ì¿¼ë¦¬ ë²¡í„° ìƒì„± ì™„ë£Œ.\n")

    # Retrieverë¥¼ í†µí•´ FAISS ê²€ìƒ‰ ë° ê²°ê³¼ ì¶œë ¥
    print("--- ìµœì¢… ë²¡í„°ë¡œ ìœ„í‚¤í”¼ë””ì•„ ë¬¸ì„œ ê²€ìƒ‰ ---")
    results = retriever.search(query_vector, top_k=config["top_k"])

    top_m = config.get("top_m", len(results))
    if VERBOSE:
        print(f"\n[DEBUG] Top-{top_m} ê²€ìƒ‰ ê²°ê³¼")
        for res in results[:top_m]:
            print(
                f"ğŸ” ìˆœìœ„ {res['rank']}: {res['title']} (ìœ ì‚¬ë„: {res['similarity']:.4f})"
            )
        print()

    # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì„œì˜ ì œëª©ì„ 'title'ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    if results:
        top_result = results[0]
        print(f"ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì œëª©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤: '{top_result['title']}'\n")
        
        #~~~~~~~~~~
        # ìˆ˜ì •ëœ ë¶€ë¶„: ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶•ì•½ ë¡œì§ ì¶”ê°€, ì„ì‹œì½”ë“œì„
        
        # 1. ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        full_text = top_result["text"]
        
        # 2. ì„¤ì • íŒŒì¼ì—ì„œ ìµœëŒ€ ê¸¸ì´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 1000 ì‚¬ìš©).
        max_len = config.get("max_context_chars", 1000)
        
        # 3. íŒŒì´ì¬ ë¬¸ìì—´ ìŠ¬ë¼ì´ì‹±ì„ ì´ìš©í•´ ì•ì—ì„œë¶€í„° max_len ë§Œí¼ë§Œ ì˜ë¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        wiki_context = full_text[:max_len]
        
        # 4. ì‚¬ìš©ìì—ê²Œ ì–´ë–»ê²Œ ì¶•ì•½ë˜ì—ˆëŠ”ì§€ ì•Œë ¤ì£¼ëŠ” ë¡œê·¸ ì¶œë ¥
        print(f"ì»¨í…ìŠ¤íŠ¸ë¥¼ {max_len}ìë¡œ ì¶•ì•½í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤:")
        print(f'"""\n{wiki_context}...\n"""\n')
    else:
        for res in results:
            print(f"ğŸ” ìˆœìœ„ {res['rank']}: {res['title']} (ìœ ì‚¬ë„: {res['similarity']:.4f})")

    # Contrieverë¥¼ ì‚¬ìš©í•œ ë¬¸ì¥ ë­í‚¹
    contriever = ContrieverReranker(device=retriever.device)
    top_sentence_k = config.get("top_m", 10)
    ranked_sentences = contriever.rank_sentences(
        config.get("vqa_question", ""), results, top_k=top_sentence_k
    )

    if VERBOSE:
        print(f"\n[DEBUG] í…ìŠ¤íŠ¸ ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ Top-{top_sentence_k} ë¬¸ì¥")
        if not ranked_sentences:
            print("ê´€ë ¨ì„± ë†’ì€ ë¬¸ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            for idx, s in enumerate(ranked_sentences):
                print(
                    f"ğŸ¯ ë¬¸ì¥ ìˆœìœ„ {idx + 1} | ì¶œì²˜ ë¬¸ì„œ: {s['source_title']} | ì„¹ì…˜: {s['section']} | ìœ ì‚¬ë„: {s['similarity']:.4f}"
                )
                print(f"   -> {s['sentence']}\n")

    wiki_context = ranked_sentences[0]["sentence"] if ranked_sentences else ""


    # VQA ëª¨ë¸ ì´ˆê¸°í™” ë° ì§ˆë¬¸ ìˆ˜í–‰

    llava = LLaVAVQA(model_id=config.get("vlm_model_id", "llava-hf/llava-1.5-7b-hf"))
    '''
    answer = llava.answer(
        final_image_to_encode, config.get("vqa_question", ""), wiki_context
    )
    '''
    answer = llava.answer(
        original_image, config.get("vqa_question", ""), wiki_context
    )
    print("VQA Answer:", answer)


if __name__ == "__main__":
    main()